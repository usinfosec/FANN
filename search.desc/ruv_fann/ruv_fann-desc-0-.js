searchState.loadedDescShard("ruv_fann", 0, "Pure Rust implementation of the Fast Artificial Neural …\nEnumeration of available training algorithms\nCascade Correlation Training for Dynamic Network Topology …\nComprehensive error handling system for ruv-FANN\nReturns the argument unchanged.\nIntegration utilities for combining all agent …\nCalls <code>U::from(self)</code>.\nI/O and serialization module for rUv-FANN\nMock types for testing I/O functionality\nHelper macros for error creation with context\nTraining algorithms for neural networks\nWebGPU compute backend for ruv-FANN neural networks\nActivation functions available for neurons\nCosine activation: f(x) = cos(x * steepness) / 2 + 0.5 …\nSymmetric cosine: f(x) = cos(x * steepness) Output range: […\nElliott activation: f(x) = ((x * steepness) / 2) / (1 + |x …\nSymmetric Elliott: f(x) = (x * steepness) / (1 + |x * …\nGaussian activation: f(x) = exp(-x * steepness * x * …\nSymmetric gaussian: f(x) = exp(-x * steepness * x * …\nLinear activation function: f(x) = x * steepness\nBounded linear: f(x) = max(0, min(1, x * steepness)) …\nSymmetric bounded linear: f(x) = max(-1, min(1, x * …\nRectified Linear Unit (ReLU): f(x) = max(0, x) Output …\nLeaky ReLU: f(x) = x if x &gt; 0, 0.01 * x if x &lt;= 0 Output …\nSigmoid activation function: f(x) = 1 / (1 + exp(-2 * …\nSymmetric sigmoid (tanh): f(x) = tanh(steepness * x) …\nSine activation: f(x) = sin(x * steepness) / 2 + 0.5 …\nSymmetric sine: f(x) = sin(x * steepness) Output range: […\nHyperbolic tangent: alias for SigmoidSymmetric\nThreshold activation function: f(x) = 0 if x &lt; 0, 1 if x …\nSymmetric threshold: f(x) = -1 if x &lt; 0, 1 if x &gt;= 0 Note: …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns whether this activation function can be used …\nReturns the string name of the activation function\nReturns the output range of the activation function\nCandidate neuron for cascade correlation\nCascade correlation builder for easy configuration\nConfiguration for cascade correlation training\nCascade correlation specific errors\nPerformance metrics for cascade training\nA network that supports cascade correlation training\nState information for cascade training\nCascade correlation trainer\nTraining record for cascade correlation\nResult of cascade correlation training\nActivation function\nCalculate the derivative of the activation function\nBest correlation achieved\nBest error achieved\nBias weight\nBias gradient\nMomentum term for bias\nCalculate correlation with provided data\nHelper methods for network structure manipulation\nCalculate correlation between candidate output and …\nCalculate the output of this candidate neuron\nCalculate network residuals (errors) for candidate training\nActivation functions to try for candidates\nLearning rate for candidate training\nMaximum epochs for candidate training\nTarget correlation for stopping candidate training\nWeight range for candidate initialization\nConfiguration parameters\nConfiguration for cascade training\nCurrent correlation score\nCurrent epoch\nCurrent training error\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate initial candidate neurons\nCurrent hidden neuron count\nNumber of hidden neurons added\nInstall a candidate neuron into the network\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum number of hidden neurons to add\nPerformance metrics\nMinimum correlation improvement to accept candidate\nMomentum coefficient\nCurrent network being trained\nThe underlying network\nCreate a new cascade trainer\nCreate a new cascade network from a base network\nCreate a new candidate neuron with random weights\nNumber of candidate neurons to train in parallel\nCurrent output value\nLearning rate for output training\nMaximum epochs for output training\nTarget error for stopping output training\nWhether to enable parallel candidate training\nPatience for early stopping (epochs without improvement)\nCalculate Pearson correlation coefficient\nRandom seed for reproducible results\nRandom number generator\nCurrent cascade training state\nActivation steepness\nMain cascade training loop\nGenerate and train candidate neurons\nTrain candidates in parallel\nTrain candidates sequentially\nTrain output weights for one epoch\nTrain output weights using standard backpropagation\nTrain a single candidate neuron\nTrain single candidate with provided data (thread-safe)\nTraining data\nTraining history\nTraining history\nUpdate candidate weights (simplified for parallel)\nUpdate weights using gradient descent with optional …\nWhether to use momentum\nWhether to use weight decay\nConfiguration validation\nTraining data validation\nVerbose logging\nWeight decay coefficient\nGradient for weight updates\nMomentum terms for weights\nWeights connecting to all previous layers and inputs\nRepresents a connection between two neurons with a weight\nReturns the argument unchanged.\nIndex of the source neuron\nCalls <code>U::from(self)</code>.\nCreates a new connection between two neurons\nSets the weight to a specific value\nIndex of the destination neuron\nUpdates the weight of the connection\nThe weight of the connection\nAbort the entire process\nActivation function problems\nLearning algorithm failures\nCandidate neuron generation issues\nCandidate selection problems\nCandidate training failures\nCascade correlation specific errors\nCascade correlation error categories\nCascade parameter validation\nFANN compatibility errors\nNeuron connection issues\nConvergence problems\nCorrelation calculation issues\nTraining data I/O problems\nContains the error value\nComprehensive error category enum for uniform handling\nError context for providing additional debugging …\nProfessional error logging and debugging facilities\nUse fallback implementation\nFile reading/writing issues\nFormat compatibility issues\nGradient calculation issues\nInput data validation\nI/O and serialization errors\nI/O error categories\nEpoch and iteration errors\nLayer configuration problems\nLearning rate problems\nMemory allocation and management errors\nNetwork configuration and topology errors\nNetwork configuration validation\nNetwork error categories for detailed classification\nNetwork export/import errors\nContains the success value\nOutput data validation\nOutput training problems\nParallel processing and concurrency errors\nPerformance and optimization errors\nForward propagation errors\nError recovery context\nError recovery strategies\nReset to a known good state\nRetry the operation with the same parameters\nRetry with modified parameters\nMain error type for all ruv-FANN operations\nComprehensive result type for all ruv-FANN operations\nSerialization/deserialization problems\nSkip the problematic operation\nStop criteria issues\nInvalid network topology or structure\nNetwork topology modification errors\nTraining and learning algorithm errors\nTraining error categories\nTraining parameter validation\nData validation and format errors\nValidation error for detailed parameter checking\nValidation error categories\nWeight and bias configuration issues\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nBenchmark result for a specific test\nIndividual compatibility test\nFANN compatibility validator\nIntegration test configuration\nIntegration test suite errors\nIntegration test result\nComprehensive integration test suite\nPerformance regression detector\nCalculate overall scores\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate test datasets\nGenerate test networks for integration testing\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLoad baseline metrics for performance comparison\nMaximum test duration per component\nCreate a new integration test suite\nPerformance regression threshold (percentage)\nRandom seed for reproducible tests\nRun the complete integration test suite\nRun a basic network functionality test\nWhether to run performance benchmarks\nRun cascade integration test\nRun performance benchmarks\nRun stress tests\nWhether to run stress tests\nRun training integration test\nTest basic network functionality across all implementations\nTest cascade correlation integration\nTest cross-agent compatibility\nTest FANN compatibility\nWhether to run FANN compatibility tests\nTest I/O system integration\nWhether to test parallel execution\nTest parallel execution\nTest training algorithm integration\nVerbose output\nBinary format (using bincode)\nCompressed binary format\nCompressed FANN format\nDecompression error\nDOT format for visualization\nDOT format exporter for network visualization\nContains the error value\nNative FANN format (text-based)\nFANN file format reader\nFANN file format writer\nSupported file formats\nInvalid file format\nInvalid network structure\nInvalid training data\nI/O error (file not found, permission denied, etc.)\nError types for I/O operations\nResult type for I/O operations\nJSON format\nContains the success value\nParse error\nSerialization error\nTraining data file format reader\nStreaming reader for large training datasets\nTraining data file format writer\nBinary serialization support using bincode\nCompress data using gzip\nCompression support for file formats\nDecompress gzip data\nDOT format export for network visualization\nError types for I/O operations\nFANN native file format reader and writer\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nJSON serialization support\nGraph layout direction\nRead binary data from a reader\nRead JSON data from a reader\nShow neuron indices\nShow weights on edges\nStreaming I/O for large datasets\nTraining data file format reader and writer\nWrite binary data to a writer\nWrite JSON data to a writer\nBinary format configuration\nBinary reader with configuration\nBinary writer with configuration\nCreate a config optimized for size (variable length …\nCreate a config optimized for speed (fixed length encoding)\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nUtilities for binary format inspection\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nUse little endian byte order\nCreate a new binary reader with default config\nCreate a new binary writer with default config\nCreate a new binary config with default settings\nRead data from a reader\nRead binary data from a reader\nRead data with size limit to prevent memory exhaustion\nGet the size of serialized data without writing\nUse variable length encoding for integers\nCreate a new binary reader with custom config\nCreate a new binary writer with custom config\nWrite data to a writer\nWrite binary data to a writer\nGet the size of data when serialized\nCheck if data can be serialized without errors\nCompression wrapper for readers\nCompression wrapper for writers\nCompression configuration\nUtilities for compression analysis\nCreate a config optimized for compression ratio\nCompress data from bytes to bytes\nCompress data using gzip\nDecompress data from bytes to bytes\nDecompress gzip data\nCreate a config optimized for speed\nUse faster compression algorithm\nFinish compression and return the inner writer\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet a mutable reference to the inner reader\nGet a mutable reference to the inner writer\nGet a reference to the inner reader\nGet a reference to the inner writer\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConsume the reader and return the inner reader\nCompression level (0-9, where 9 is best compression)\nCreate a new compressed reader\nCreate a new compressed writer with default compression\nCreate a new compression config with default settings\nCreate a new compressed writer with config\nCreate a new compressed writer with custom compression …\nCreate a config with custom compression level\nCompression statistics\nCalculate compression ratio\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalculate space savings percentage\nTest compression effectiveness on data\nBottom to top\nDOT format exporter for network visualization\nGraph layout direction\nLeft to right (default for neural networks)\nRight to left\nTop to bottom\nExport a neural network to DOT format\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nGraph layout direction\nCreate a new DOT exporter with default settings\nShow neuron indices\nShow weights on edges\nCreate a new DOT exporter with custom settings\nDecompression error\nContains the error value\nInvalid file format\nInvalid network structure\nInvalid training data\nI/O error (file not found, permission denied, etc.)\nError types for I/O operations\nResult type for I/O operations\nContains the success value\nParse error\nSerialization error\nFANN file format reader\nFANN file format writer\nCreate a new FANN reader\nCreate a new FANN writer\nRead a neural network from a FANN format file\nWrite a neural network to FANN format\nJSON format configuration\nJSON reader with configuration\nJSON writer with configuration\nCreate a compact JSON config (no pretty printing)\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInclude null values in serialization\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new JSON reader with default config\nCreate a new JSON writer with default config\nCreate a new JSON config with default settings\nCreate a pretty JSON config (with indentation)\nUse pretty printing (indented, human-readable)\nRead data from a reader\nRead JSON data from a reader\nRead JSON data from a reader with custom options\nCreate a new JSON reader with custom config\nCreate a new JSON writer with custom config\nWrite data to a writer\nWrite JSON data to a writer\nWrite JSON data to a writer with custom options\nBuffered reader wrapper for streaming\nStatistics from streaming operations\nStreaming reader for training data\nCalculate average bytes per sample\nGet the number of bytes in the internal buffer\nGet the buffer size\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nUtilities for memory-efficient streaming\nCreate a new streaming reader with default buffer size\nCreate a new buffered stream reader\nCalculate total parameters (inputs + outputs) per sample\nRead training data in batches\nRead training data with a callback for each sample\nCreate a new streaming reader with custom buffer size\nCreate a new buffered stream reader with custom buffer size\nEstimate memory usage for batch processing\nCalculate optimal batch size for given memory limit\nTraining data file format reader\nStreaming reader for large training datasets\nTraining data file format writer\nCreate a new training data reader\nCreate a new training data writer\nCreate a new streaming reader\nRead training data from a FANN data format file\nRead training data with a callback for each sample\nWrite training data to FANN data format\nRepresents a layer of neurons in the neural network\nGets a reference to the bias neuron if it exists\nGets a mutable reference to the bias neuron if it exists\nCalculates outputs for all neurons in the layer based on …\nConnects all neurons in this layer to all neurons in the …\nReturns the argument unchanged.\nGets the output values of all neurons in the layer\nChecks if the layer has a bias neuron\nCalls <code>U::from(self)</code>.\nThe neurons in this layer\nCreates a new layer with the specified number of neurons\nReturns the number of regular neurons (excluding bias)\nResets all neurons in the layer\nSets the activation function for all neurons in the layer …\nSets the activation steepness for all neurons in the layer …\nSets the values of neurons in the layer (used for input …\nReturns the number of neurons in the layer (including bias …\nCreates a new layer with a bias neuron\nMock network structure for testing\nMock training data structure for testing\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nA feedforward neural network\nBuilder for creating neural networks with a fluent API\nErrors that can occur during network operations\nBuilds the network\nSets the connection rate (0.0 to 1.0)\nConnection rate (1.0 = fully connected, 0.0 = no …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nDeserialize a network from bytes\nAlias for total_connections for compatibility\nGets all weights in the network as a flat vector\nAdds a hidden layer with default activation (Sigmoid)\nAdds a hidden layer with specific activation function\nAdds an input layer to the network\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe layers of the network\nCreate layers from a slice of layer sizes\nCreates a new network builder\nCreates a new network with the specified layer sizes\nReturns the number of input neurons (excluding bias)\nReturns the number of layers in the network\nReturns the number of output neurons\nAdds an output layer with default activation (Sigmoid)\nAdds an output layer with specific activation function\nRandomizes all weights in the network within the given …\nResets all neurons in the network\nRuns a forward pass through the network\nRun batch inference on multiple inputs\nSets the activation function for all neurons in a specific …\nSets the activation function for all hidden layers\nSets the activation function for the output layer\nSets the activation steepness for all hidden layers\nSets the activation steepness for the output layer\nSets the training algorithm (placeholder for API …\nSets all weights in the network from a flat vector\nSerialize the network to bytes\nReturns the total number of connections in the network\nReturns the total number of neurons in the network\nTrain the network with the given data\nRepresents a single neuron in the neural network\nThe activation function to use\nThe steepness parameter for the activation function\nAdds a connection from another neuron to this neuron\nCalculates the neuron’s output based on inputs and …\nClears all connections\nIncoming connections to this neuron\nReturns the argument unchanged.\nGets the weight of a specific connection by index\nCalls <code>U::from(self)</code>.\nWhether this is a bias neuron\nCreates a new neuron with the specified activation …\nCreates a new bias neuron with a constant output value of …\nResets the neuron’s sum and value to zero (except for …\nSets the weight of a specific connection by index\nSets the neuron’s output value directly (used for input …\nThe sum of inputs multiplied by weights\nThe output value after applying the activation function\nBatch backpropagation Accumulates gradients over entire …\nBit fail based stop criteria\nTrait for error/loss functions\nExponential decay learning rate schedule\nIncremental (online) backpropagation Updates weights after …\nLearning rate schedule trait\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nMSE-based stop criteria\nOptions for parallel training\nQuickprop trainer An advanced batch training algorithm …\nRPROP (Resilient Propagation) trainer An adaptive learning …\nStep decay learning rate schedule\nStop criteria trait\nTanh Error Function\nMain trait for training algorithms\nCallback function type for training progress\nError types for training operations\nTraining state that can be saved and restored\nBackpropagation training algorithms\nBatch size for parallel processing\nCalculate the error between actual and desired outputs\nCalculate the current error\nCall the callback if set\nCount bit fails\nCalculate the derivative of the error function\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nHelper functions for forward propagation and gradient …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nNumber of threads to use (0 = use all available cores)\nWhether to use parallel error calculation\nWhether to use parallel gradient computation\nQuickprop training algorithm\nRestore training state\nResilient Propagation (RPROP) training algorithm\nSave training state\nSet a callback function\nTrain for one epoch\nBatch backpropagation Accumulates gradients over entire …\nIncremental (online) backpropagation Updates weights after …\nSimple network representation for training algorithms\nApply weight and bias updates back to the real Network\nCalculate gradients using backpropagation on simplified …\nForward propagation through the simplified network\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConvert a real Network to a simplified representation for …\nActivation function that works with our simplified …\nSigmoid derivative\nQuickprop trainer An advanced batch training algorithm …\nRPROP (Resilient Propagation) trainer An adaptive learning …\nMemory management capabilities\nAutonomous GPU Resource Management System\nCore compute backend trait and implementations\nAdvanced 5-Tier Buffer Pooling System with DAA Integration\nComputeContext bridge for <code>Network&lt;T&gt;</code> integration with …\nGPU device management with advanced capabilities detection\nError types for WebGPU compute backend\nRobust fallback system ensuring graceful degradation\nReturns the argument unchanged.\nGet memory management capabilities summary\nCheck if enhanced memory management features are available\nCalls <code>U::from(self)</code>.\nGPU Kernel Optimization System Advanced workgroup sizing, …\nGPU memory management with intelligent buffer pooling\nWebGPU Performance Monitoring System Real-time performance …\nWebGPU Pipeline Cache and Compilation System Optimized …\nReal-time Memory Pressure Monitoring System with DAA …\nWebGPU shader definitions and compilation\nGet capabilities summary string\nAdvanced WebGPU compute backend implementation\nIndividual agent’s resource allocation with autonomous …\nAllocation engine responsible for intelligent resource …\nCentral autonomous GPU resource manager that orchestrates …\nConflict resolution system for handling resource contention\nEvent bus for autonomous coordination and communication\nOptimization engine for continuous performance improvement\nPerformance analysis system for continuous optimization\nResource market for dynamic pricing and trading\nResource pool representing a collection of GPU resources …\nResource trading system enabling agent-to-agent resource …\nrUv token ledger for economic coordination\nUsage prediction system for proactive resource management\nForward declarations for types used in main structs\nCurrent allocations and reservations\nActive conflicts\nPerformance tracking\nAdaptive configuration\nAllocation algorithms\nLearning system\nPerformance tracking\nPool policies\nAutonomous behaviors\nApply conflict resolution\nAutonomous behaviors\nAccount balances\nEconomic properties\nBottleneck detection\nEvent channels\nEconomic data\nRisk management\nAutonomous conflict detection and resolution\nEvent system for autonomous coordination\nEvent history\nEvent processing rules\nExecute a trade between agents\nFairness and equity tracking\nFeature engineering\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet current resource utilization across all pools\nEconomic incentives\nInitialize default resource pools\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMarket makers and liquidity\nAutonomous market making\nCore components\nPerformance metrics collection\nPrediction models\nCreate a new autonomous GPU resource manager\nActive trades and market\nPerformance history\nPerformance monitoring\nPool characteristics\nConfiguration and policies\nPrediction accuracy tracking\nMarket data\nTrading algorithms\nAllocation metadata\nAutonomous resource trading between agents\nOptimization recommendations\nRequest resource allocation with autonomous optimization\nResolution history for learning\nResolution strategies\nResource pools and tracking\nEconomic tracking\nEconomic system\nStart autonomous background tasks\nOptimization strategies\nBackground task management\nAgent-specific metrics\nEconomic parameters\nTransaction history\nContinuous autonomous optimization\nHistorical data\nLearning and adaptation\nAutonomous learning and prediction\nUsage tracking\nMarket dynamics\nPredictive optimization\nIntelligent backend selection with performance-based …\nAbstract compute backend for neural network operations\nCPU fallback backend\nMemory management interface\nSIMD backend using existing ruv-FANN SIMD operations\nVector operation primitives\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet available backend types\nGet current active backend type\nBackend initialization and capability detection\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCore neural network operations\nSelect optimal backend for given problem size\nSet the active backend\nAdvanced 5-tier buffer pool with DAA integration\nBuffer size categories for optimal pooling\nPool for a specific buffer tier with enhanced analytics\nDAA resource management recommendation\nGPU buffer with enhanced metadata and lifecycle tracking\nMemory pressure levels\nGlobal pool statistics across all tiers\nSnapshot of pool statistics for monitoring\nPool configuration for a specific buffer tier\nCircuit breaker for memory pressure protection\nStatistics for individual buffer tier\nApply DAA recommendations for autonomous optimization\nCalculate overall cache hit ratio\nCalculate current memory pressure\nCleanup old buffers based on memory pressure\nGet expected allocation latency for this tier\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate DAA recommendations for pool optimization\nGet buffer with sub-millisecond allocation for cached …\nGet DAA priority score (0.0-1.0)\nGet performance score (0.0-1.0)\nGet comprehensive pool statistics\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalculate memory efficiency ratio\nGet performance summary\nGet optimal pool configuration for this tier\nReturn buffer to pool with DAA optimization\nCalculate reuse efficiency score for DAA optimization\nUpdate DAA priority score (0-1000)\nDAA-enhanced buffer retention decision\nGet size range for this category\nTry to coalesce buffers into a larger one\nUpdate performance score based on usage patterns\nComputeContext manages backend selection and operation …\nComprehensive performance statistics\nDAA coordination metrics\nPerformance tracking for optimization decisions\nPerformance tracker statistics\nCPU activation function implementation\nBackend selector for intelligent backend switching\nCalculate optimization score for DAA coordination\nClear weight cache (call when network weights change)\nCPU fallback layer computation\nExecute forward pass for a layer with optimal backend …\nGPU-accelerated layer computation\nSIMD-optimized layer computation\nExecute complete network forward pass with optimal backend …\nCreate a compute context with CPU-only backend (for …\nGet current backend type\nCurrent backend type being used\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet backend switch count for DAA coordination\nGet DAA coordination metrics\nConvert Network layer to matrix format with caching\nGet comprehensive performance statistics\nGPU acceleration enabled flag\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if GPU acceleration is available\nCreate a new compute context with automatic backend …\nPerformance tracking and optimization\nSelect optimal backend for given problem size\nWebGPU backend instance (when available)\nCache for converted weights to avoid repeated conversions\nDetailed device information for optimization\nDevice type classification for optimization\nGPU device wrapper with advanced capabilities\nDevice adapter information\nCreate optimal bind group layout for activation functions\nCreate compute shader with error handling and optimization\nCreate optimal bind group layout for matrix operations\nWebGPU device handle\nEstimate optimal workgroup size for given problem size\nGet estimated compute throughput in GFLOPS\nGet estimated memory bandwidth in GB/s\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet comprehensive device information\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if device is suitable for high-performance computing\nDevice limits and capabilities\nGet maximum buffer size supported by this device\nGet maximum compute workgroup size in each dimension\nGet maximum number of compute workgroups per dimension\nGet maximum storage buffer binding size\nInitialize GPU device with advanced capability detection\nWebGPU queue for command submission\nSubmit command buffers with performance tracking\nCheck device features for optimization decisions\nValidate device capabilities for neural network operations\nWait for all submitted work to complete\nErrors that can occur during compute operations\nResult type for compute operations\nContains the error value\nContains the success value\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nFallback manager with circuit breaker pattern\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGPU device capabilities and optimization parameters\nOptimized kernel configuration for specific operations\nAdvanced kernel optimizer with GPU-specific optimizations\nPerformance optimization metrics\nAuto-tune workgroup size by testing different …\nClear optimization caches (useful for testing or GPU …\nNumber of compute units/streaming multiprocessors\nCompute utilization (0.0 to 1.0)\nCached optimal configurations for different operations\nNumber of elements processed per thread\nEstimated execution time in microseconds\nEstimated performance in GFLOPS\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet GPU capabilities\nGPU capabilities for this device\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum shared memory per workgroup in bytes\nMaximum number of threads per workgroup\nMaximum workgroup size in each dimension\nMemory bandwidth in GB/s\nMemory access efficiency\nMemory bandwidth utilization (0.0 to 1.0)\nCreate a new kernel optimizer with detected GPU …\nNumber of workgroups to dispatch\nOccupancy (threads per SM / max threads per SM)\nOptimize configuration for activation functions\nOptimize configuration for batch matrix-vector operations\nOptimize kernel configuration for matrix-vector …\nPerformance history for continuous optimization\nGet performance predictions based on historical data\nPreferred workgroup size for this GPU architecture\nRecord performance metrics for continuous optimization\nSubgroup/wavefront size (typically 32 or 64)\nWhether the GPU supports subgroups/wavefronts\nTile size for tiled algorithms\nWhether to use vectorized memory access\nCreate with default capabilities (fallback for unknown …\nOptimal workgroup size for this kernel\nCPU-side memory manager for fallback when GPU is not …\nEnhanced GPU memory manager with DAA integration\nEnhanced memory statistics combining legacy and advanced …\nConfiguration for enhanced GPU memory management\nGPU memory manager that automatically selects between …\nOptimization result information\nAllocate buffer with enhanced allocation strategy\nAutomatically start monitoring on initialization\nGet average allocation latency in nanoseconds\nGet cache hit ratio across all buffer pools\nPerform memory cleanup\nCreate readback buffer\nCreate storage buffer with specific usage\nCreate uniform buffer\nDeallocate buffer\nEnable advanced buffer pooling and DAA features\nEnable circuit breaker protection\nEnable DAA autonomous optimization\nEnable real-time pressure monitoring\nEnable performance optimization features\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet monitoring report\nGet current memory statistics\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if using advanced features\nMaximum allocation latency before circuit breaker trips\nConfiguration for pressure monitoring\nCreate new enhanced GPU memory manager\nOptimize memory layout for DAA coordination\nGenerate performance summary string\nGet current memory pressure as a ratio (0.0-1.0)\nMemory pressure threshold for triggering cleanup (0.0-1.0)\nStart memory pressure monitoring\nStop memory pressure monitoring\nGet total allocated memory in bytes\nGet uptime duration\nWebGPU memory management module\nCreate enhanced memory manager with custom configuration\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nPerformance measurement for a single GPU operation\nReal-time performance monitoring system\nPerformance statistics aggregated over multiple runs\nPerformance alert thresholds\nAverage compute throughput\nAverage execution time\nAverage GPU occupancy\nAverage memory bandwidth\nClear all performance data\nCompute throughput achieved (GFLOPS)\nCurrent operations per second\nData size processed\nActual execution time\nStandard deviation of execution times\nExport performance data for analysis\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet performance trend for an operation\nGet real-time performance metrics\nGet performance statistics for an operation\nGPU occupancy (0.0 to 1.0)\nGPU utilization percentage\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum execution time (worst case)\nMaximum acceptable execution time degradation (ratio)\nMaximum acceptable execution time variance\nMaximum number of measurements to keep per operation\nRaw performance measurements\nMemory bandwidth achieved (GB/s)\nMemory efficiency (0.0 to 1.0)\nMinimum execution time (best case)\nMinimum acceptable GPU occupancy\nMinimum acceptable memory bandwidth utilization\nMinimum samples required for reliable statistics\nCreate a new performance monitor with default settings\nGet performance prediction for an operation\nReal-time metrics\nRecord a performance measurement\nAverage compute throughput over last 100 operations\nAverage memory bandwidth over last 100 operations\nNumber of measurements\nShader type used\nAggregated statistics\nTimestamp of measurement\nTotal GPU time used\nTotal operations executed\nPerformance trend (improving, stable, degrading)\nCreate with custom configuration\nHigh-performance pipeline cache with optimized compilation …\nCached bind group layouts to avoid recompilation\nCache hit/miss statistics\nClear cache and reset statistics (useful for testing)\nCompilation statistics for performance monitoring\nCompile a shader pipeline (placeholder implementation)\nCreate bind group layout for shader type (placeholder …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cache hit ratio for performance monitoring\nGet or compile a compute pipeline with automatic caching\nGet or create a bind group layout with caching\nGet comprehensive cache performance statistics\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new pipeline cache with optimized settings\nCompiled compute pipelines indexed by shader type\nPrecompile commonly used shaders for optimal startup …\nAdaptation parameters for DAA learning\nAlert severity levels\nAlert thresholds for different pressure levels\nAnomaly detection system\nAnomaly event record\nTypes of memory anomalies\nAutonomous response record\nBaseline memory behavior statistics\nCircuit breaker state for memory operations\nDAA coordination for autonomous memory management\nDAA decision record\nDAA performance metrics\nDetection model for specific anomaly type\nDAA learning engine for strategy optimization\nReal-time memory pressure monitor with DAA integration\nState for each prediction model\nMonitor configuration\nComprehensive monitoring report\nMonitoring statistics\nPerformance record for learning\nAvailable prediction models\nMethods for calculating memory pressure\nPressure prediction with confidence intervals\nPressure prediction engine\nIndividual pressure reading with metadata\nPossible response actions\nResponse strategy for each pressure level\nSuccess criteria for response strategies\nCollect single pressure reading\nGet current pressure reading\nDetect memory anomalies\nEvaluate if autonomous response is needed\nExecute autonomous response\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate pressure prediction\nGenerate system recommendations\nGenerate monitoring report\nGet DAA performance metrics\nGet monitoring statistics\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet latest pressure prediction\nCreate new pressure monitor with DAA integration\nGet recent anomalies\nStart monitoring with background thread\nStop monitoring\nGet summary string for logging\nUpdate baseline statistics for anomaly detection\nUpdate alert thresholds\nEmbedded shader source code\nActivation functions shader\nAdvanced neural network operations shader\nBatch matrix-vector multiplication shader\nGradient operations shader for backpropagation\nMatrix-vector multiplication shader\nGet shader type for activation function\nClear all caches (useful for testing)\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet bind group layout with caching\nGet cache hit ratio for monitoring\nGet optimized kernel configuration for operation\nGet performance statistics from caches\nGet or compile a compute pipeline with automatic caching\nGet shader source code for a given shader type\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nKernel optimizer for workgroup sizing and performance …\nCreate a new shader manager with pipeline caching and …\nPerformance monitoring enabled\nPipeline cache for optimized shader compilation and reuse\nGet performance predictions\nRecord performance metrics for optimization\nPrecompile commonly used shaders for optimal performance\nCreate with custom GPU capabilities for optimization\nWebGPU compute backend\nPhantom data for type safety\nAllocate GPU buffer with size validation and memory …\nApply activation function with GPU acceleration\nPerform batch matrix-vector multiplication with GPU …\nGPU device capabilities and limits\nOptimized CPU activation function application\nOptimized CPU batch matrix-vector multiplication\nOptimized CPU dot product with vectorization hints\nOptimized CPU matrix-vector multiplication\nDeallocate GPU buffer with memory pool management\nDetect actual device capabilities asynchronously\nCompute dot product of two vectors with GPU acceleration\nDownload data from GPU buffer with transfer optimization\nReturns the argument unchanged.\nInitialize WebGPU backend asynchronously\nCalls <code>U::from(self)</code>.\nCheck if WebGPU is available on the current platform\nPerform matrix-vector multiplication using GPU acceleration\nGet current memory usage statistics\nWGSL shader compiler and manager\nUpload data to GPU buffer with transfer optimization\nValidate that device capabilities meet minimum requirements\nElement-wise vector addition with GPU acceleration\nScale vector by scalar with GPU acceleration\nElement-wise vector subtraction with GPU acceleration")