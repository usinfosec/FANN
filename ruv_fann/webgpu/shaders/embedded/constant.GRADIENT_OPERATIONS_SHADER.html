<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Gradient operations shader for backpropagation"><title>GRADIENT_OPERATIONS_SHADER in ruv_fann::webgpu::shaders::embedded - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../../../../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../../../../static.files/rustdoc-1a91846b.css"><meta name="rustdoc-vars" data-root-path="../../../../" data-static-root-path="../../../../static.files/" data-current-crate="ruv_fann" data-themes="" data-resource-suffix="" data-rustdoc-version="1.88.0 (6b00bc388 2025-06-23)" data-channel="1.88.0" data-search-js="search-f7877310.js" data-settings-js="settings-5514c975.js" ><script src="../../../../static.files/storage-4e99c027.js"></script><script defer src="sidebar-items.js"></script><script defer src="../../../../static.files/main-7ef8a74a.js"></script><noscript><link rel="stylesheet" href="../../../../static.files/noscript-893ab5e7.css"></noscript><link rel="alternate icon" type="image/png" href="../../../../static.files/favicon-32x32-6580c154.png"><link rel="icon" type="image/svg+xml" href="../../../../static.files/favicon-044be391.svg"></head><body class="rustdoc constant"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="mobile-topbar"><button class="sidebar-menu-toggle" title="show sidebar"></button></nav><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../../../../ruv_fann/index.html">ruv_<wbr>fann</a><span class="version">0.1.6</span></h2></div><div class="sidebar-elems"><div id="rustdoc-modnav"><h2><a href="index.html">In ruv_<wbr>fann::<wbr>webgpu::<wbr>shaders::<wbr>embedded</a></h2></div></div></nav><div class="sidebar-resizer"></div><main><div class="width-limiter"><rustdoc-search></rustdoc-search><section id="main-content" class="content"><div class="main-heading"><div class="rustdoc-breadcrumbs"><a href="../../../index.html">ruv_fann</a>::<wbr><a href="../../index.html">webgpu</a>::<wbr><a href="../index.html">shaders</a>::<wbr><a href="index.html">embedded</a></div><h1>Constant <span class="constant">GRADIENT_OPERATIONS_SHADER</span><button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../../../../src/ruv_fann/webgpu/shaders.rs.html#17">Source</a> </span></div><pre class="rust item-decl"><code>pub const GRADIENT_OPERATIONS_SHADER: &amp;<a class="primitive" href="https://doc.rust-lang.org/1.88.0/std/primitive.str.html">str</a> = &quot;// WebGPU Compute Shaders: Gradient Operations for Backpropagation\n// High-performance GPU implementations of gradient computations\n// Optimized for neural network training with vectorized operations\n\n@group(0) @binding(0) var&lt;storage, read&gt; gradients_output: array&lt;f32&gt;;\n@group(0) @binding(1) var&lt;storage, read&gt; activations: array&lt;f32&gt;;\n@group(0) @binding(2) var&lt;storage, read&gt; weights: array&lt;f32&gt;;\n@group(0) @binding(3) var&lt;storage, read_write&gt; gradients_input: array&lt;f32&gt;;\n@group(0) @binding(4) var&lt;storage, read_write&gt; weight_gradients: array&lt;f32&gt;;\n\nstruct GradientUniforms {\n    input_size: u32,\n    output_size: u32,\n    batch_size: u32,\n    learning_rate: f32,\n    steepness: f32,       // For activation derivative calculation\n    alpha: f32,           // For parameterized functions\n    reserved: u32,\n}\n\n@group(0) @binding(5) var&lt;uniform&gt; uniforms: GradientUniforms;\n\n// Sigmoid derivative: f\&#39;(x) = f(x) * (1 - f(x)) where f(x) is sigmoid output\n@compute @workgroup_size(256)\nfn sigmoid_gradient_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.output_size * uniforms.batch_size) {\n        return;\n    }\n    \n    let sigmoid_output = activations[index];\n    let derivative = sigmoid_output * (1.0 - sigmoid_output) * uniforms.steepness;\n    gradients_input[index] = gradients_output[index] * derivative;\n}\n\n// ReLU derivative: f\&#39;(x) = 1 if x &gt; 0, 0 if x &lt;= 0\n@compute @workgroup_size(256)\nfn relu_gradient_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.output_size * uniforms.batch_size) {\n        return;\n    }\n    \n    let derivative = select(0.0, 1.0, activations[index] &gt; 0.0);\n    gradients_input[index] = gradients_output[index] * derivative;\n}\n\n// Leaky ReLU derivative: f\&#39;(x) = 1 if x &gt; 0, alpha if x &lt;= 0\n@compute @workgroup_size(256)\nfn leaky_relu_gradient_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.output_size * uniforms.batch_size) {\n        return;\n    }\n    \n    let derivative = select(uniforms.alpha, 1.0, activations[index] &gt; 0.0);\n    gradients_input[index] = gradients_output[index] * derivative;\n}\n\n// Tanh derivative: f\&#39;(x) = steepness * (1 - f(x)^2) where f(x) is tanh output\n@compute @workgroup_size(256)\nfn tanh_gradient_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.output_size * uniforms.batch_size) {\n        return;\n    }\n    \n    let tanh_output = activations[index];\n    let derivative = uniforms.steepness * (1.0 - tanh_output * tanh_output);\n    gradients_input[index] = gradients_output[index] * derivative;\n}\n\n// Linear derivative: f\&#39;(x) = steepness (constant)\n@compute @workgroup_size(256)\nfn linear_gradient_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.output_size * uniforms.batch_size) {\n        return;\n    }\n    \n    gradients_input[index] = gradients_output[index] * uniforms.steepness;\n}\n\n// Weight gradient computation for fully connected layers\n// Computes: dW = activation_input^T * gradient_output\n@compute @workgroup_size(16, 16)\nfn weight_gradient_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let input_idx = global_id.x;\n    let output_idx = global_id.y;\n    \n    if (input_idx &gt;= uniforms.input_size || output_idx &gt;= uniforms.output_size) {\n        return;\n    }\n    \n    let weight_idx = output_idx * uniforms.input_size + input_idx;\n    var gradient_sum: f32 = 0.0;\n    \n    // Sum gradients across all samples in the batch\n    for (var batch_idx = 0u; batch_idx &lt; uniforms.batch_size; batch_idx++) {\n        let activation_idx = batch_idx * uniforms.input_size + input_idx;\n        let grad_output_idx = batch_idx * uniforms.output_size + output_idx;\n        \n        gradient_sum += activations[activation_idx] * gradients_output[grad_output_idx];\n    }\n    \n    // Average across batch and apply learning rate\n    weight_gradients[weight_idx] = gradient_sum / f32(uniforms.batch_size) * uniforms.learning_rate;\n}\n\n// Input gradient computation for fully connected layers\n// Computes: gradient_input = weights^T * gradient_output\n@compute @workgroup_size(256)\nfn input_gradient_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let batch_input_idx = global_id.x;\n    \n    if (batch_input_idx &gt;= uniforms.input_size * uniforms.batch_size) {\n        return;\n    }\n    \n    let batch_idx = batch_input_idx / uniforms.input_size;\n    let input_idx = batch_input_idx % uniforms.input_size;\n    \n    var gradient_sum: f32 = 0.0;\n    \n    // Sum: weights[output][input] * gradient_output[batch][output]\n    for (var output_idx = 0u; output_idx &lt; uniforms.output_size; output_idx++) {\n        let weight_idx = output_idx * uniforms.input_size + input_idx;\n        let grad_output_idx = batch_idx * uniforms.output_size + output_idx;\n        \n        gradient_sum += weights[weight_idx] * gradients_output[grad_output_idx];\n    }\n    \n    gradients_input[batch_input_idx] = gradient_sum;\n}\n\n// Efficient gradient clipping to prevent exploding gradients\n@compute @workgroup_size(256)\nfn gradient_clipping_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.input_size) {\n        return;\n    }\n    \n    let gradient = gradients_input[index];\n    let clip_value = uniforms.alpha; // Reuse alpha parameter for clip threshold\n    \n    // Clip gradient to [-clip_value, clip_value]\n    gradients_input[index] = clamp(gradient, -clip_value, clip_value);\n}\n\n// L2 regularization gradient addition\n@compute @workgroup_size(256)\nfn l2_regularization_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.input_size) {\n        return;\n    }\n    \n    let weight = weights[index];\n    let l2_lambda = uniforms.alpha; // Reuse alpha for L2 regularization strength\n    \n    // Add L2 regularization term: gradient += lambda * weight\n    weight_gradients[index] += l2_lambda * weight;\n}\n\n// Momentum update for weights (SGD with momentum)\n@compute @workgroup_size(256)\nfn momentum_update_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.input_size) {\n        return;\n    }\n    \n    // Momentum stored in gradients_input buffer for this kernel\n    let momentum_decay = uniforms.alpha; // Typically 0.9\n    let gradient = weight_gradients[index];\n    let momentum = gradients_input[index];\n    \n    // Update momentum: momentum = momentum_decay * momentum + gradient\n    let new_momentum = momentum_decay * momentum + gradient;\n    gradients_input[index] = new_momentum;\n    \n    // Update weight gradients with momentum\n    weight_gradients[index] = new_momentum;\n}\n\n// Adam optimizer state update (simplified version)\n@compute @workgroup_size(256)\nfn adam_update_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let index = global_id.x;\n    \n    if (index &gt;= uniforms.input_size) {\n        return;\n    }\n    \n    let gradient = weight_gradients[index];\n    \n    // Note: In full implementation, we would need separate buffers for m and v\n    // For now, this is a placeholder for the Adam update logic\n    let beta1 = 0.9;  // First moment decay\n    let beta2 = 0.999; // Second moment decay\n    let epsilon = 1e-8;\n    \n    // m = beta1 * m + (1 - beta1) * gradient\n    // v = beta2 * v + (1 - beta2) * gradient^2\n    // gradient_adjusted = m / (sqrt(v) + epsilon)\n    \n    // Simplified update (would need proper state management in real implementation)\n    let gradient_squared = gradient * gradient;\n    let adjusted_gradient = gradient / (sqrt(gradient_squared) + epsilon);\n    \n    weight_gradients[index] = adjusted_gradient * uniforms.learning_rate;\n}\n\n// Batch normalization gradient computation\n@compute @workgroup_size(256)\nfn batch_norm_gradient_main(@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;) {\n    let feature_idx = global_id.x;\n    \n    if (feature_idx &gt;= uniforms.input_size) {\n        return;\n    }\n    \n    // Simplified batch norm gradient computation\n    // In practice, this would require mean and variance computations\n    var gradient_sum: f32 = 0.0;\n    var mean: f32 = 0.0;\n    \n    // Compute mean activation for this feature\n    for (var batch_idx = 0u; batch_idx &lt; uniforms.batch_size; batch_idx++) {\n        let activation_idx = batch_idx * uniforms.input_size + feature_idx;\n        mean += activations[activation_idx];\n    }\n    mean /= f32(uniforms.batch_size);\n    \n    // Compute gradient (simplified)\n    for (var batch_idx = 0u; batch_idx &lt; uniforms.batch_size; batch_idx++) {\n        let activation_idx = batch_idx * uniforms.input_size + feature_idx;\n        let grad_idx = batch_idx * uniforms.input_size + feature_idx;\n        \n        let centered = activations[activation_idx] - mean;\n        gradient_sum += gradients_output[grad_idx] * centered;\n    }\n    \n    // Store normalized gradient\n    gradients_input[feature_idx] = gradient_sum / f32(uniforms.batch_size);\n}&quot;;</code></pre><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>Gradient operations shader for backpropagation</p>
</div></details></section></div></main></body></html>